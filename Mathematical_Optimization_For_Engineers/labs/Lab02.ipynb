{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<div>\n<img src=\"figures/svtLogo.png\"/>\n</div>\n<center><h1>Mathematical Optimization for Engineers</h1></center>\n<center><h2>Lab 2</h2></center>\n<center><h2>Basic math</h2></center>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "$$\\newcommand{\\mr}[1]{\\mathrm{#1}}\n\\newcommand{\\D}{\\displaystyle}\n\\newcommand{\\bm}[1]{\\text{\\mathbf $#1$}}\n\\newcommand{\\bx}{\\mathbf{ x}}\n\\newcommand{\\f}{\\mathbf{{f}}}\n\\newcommand{\\g}{\\mathbf{ g}}\n\\newcommand{\\h}{\\mathbf{ h}}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\A}{\\mathbf{ A}}\n\\newcommand{\\br}{\\boldsymbol{r}}\n\\newcommand{\\bp}{\\boldsymbol{p}}\n\\newcommand{\\bnabla}{\\mathbf{\\nabla}}\n$$\nIn this lab, we will learn about Jacobians, gradients and Hessian matrices.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Notation</u>: Please note that throughout the course, we will denote matrices and vectors with boldface letters. Their components will be denoted by normal letters with subscripts. For example,\n$$\n\\bx = \\left(\\begin{array}{c}\nx_1 \\\\ \n\\vdots \\\\ \nx_n\n\\end{array} \\right)\n$$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Jacobian\nLet $\\ \\mathbf \\f:\\R^n \\rightarrow \\R^m,\\,\\bx \\mapsto \\f(\\bx)$ be a continuously differentiable function, where\n$$\n\\bx = \\left(\\begin{array}{c}\nx_1 \\\\ \n\\vdots \\\\ \nx_n\n\\end{array} \\right) , \\qquad \\f(\\bx)  = \\left(\\begin{array}{c}\nf_1(\\bx) \\\\ \n\\vdots \\\\ \nf_m(\\bx) \n\\end{array} \\right).\n$$\nThe Jacobian $\\f'(\\bx)$ is defined by the matrix\n$$\n\\f'(\\bx) = \\left(  \\begin{array}{ccc}\n\\frac{\\partial f_1(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(\\bx)}{\\partial x_n} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\frac{\\partial f_m(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(\\bx)}{\\partial x_n}\n\\end{array}  \\right).\n$$\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Gradient\n\nNow, let $f:\\R^n \\rightarrow \\R,\\,\\bx \\mapsto f(\\bx)$ be a *scalar-valued* continuously differentiable function\nwith vector-valued arguments.\nThe gradient $\\bnabla f(\\bx)$ and Jacobian $f'(\\bx)$ are defined as the column vector and row vector, respectively,\n\n$$\n\\bnabla f(\\bx)  = \\left(\\begin{array}{c}\n\t\\frac{\\partial f(\\bx)}{\\partial x_1} \\\\ \n\t\\vdots \\\\ \n\t\\frac{\\partial f(\\bx)}{\\partial x_n}\n\\end{array} \\right), \\qquad  f'(\\bx)  = \\left(\\begin{array}{ccc}  \n\\frac{\\partial f(\\bx)}{\\partial x_1} & \n\\cdots & \n\\frac{\\partial f(\\bx)}{\\partial x_n}\n\\end{array}\\right).\n$$\n\nNote that  $\\bnabla f(\\bx)^T=f'(\\bx)$.<br>\n<br>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Gradient of the scalar product of two functions\n\nLet $\\g,\\h:\\R^n\\rightarrow\\R^n$ be two continuously differentiable functions. We want to compute the\ngradient of $f:\\R^n\\rightarrow \\R,\\;\\bx \\mapsto f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, where $\\bx\\in\\R^n$.\nWe have\n\n$$\nf(\\bx) = \\g(\\bx)^T\\h(\\bx) = \\sum_{i=1}^{n} g_i(\\bx)h_i(\\bx).\n$$\n\nThe derivative with respect to $x_j$ ($1 \\le j \\le n$) is computed by the application of the product rule\n\n\\begin{equation}\\label{eq:1}\n\\frac{\\partial f(\\bx)}{\\partial x_j} = \\sum_{i=1}^{n} \\left(\\frac{\\partial g_i(\\bx)}{\\partial x_j}h_i(\\bx)  +g_i(\\bx)\\frac{\\partial h_i(\\bx)}{\\partial x_j}\\right).\n\\end{equation}\n\nWith the notations\n\n$$\n\\frac{\\partial \\g(\\bx)}{\\partial x_j}   = \\left(\\begin{array}{c}\n\\frac{\\partial g_1(\\bx)}{\\partial x_j} \\\\ \n\\vdots \\\\ \n\\frac{\\partial g_n(\\bx)}{\\partial x_j}\n\\end{array} \\right) \\quad \\text{and} \\quad \n\\frac{\\partial \\h(\\bx)}{\\partial x_j}   \n= \\left(\\begin{array}{c}\n\\frac{\\partial h_1(\\bx)}{\\partial x_j} \\\\ \n\\vdots \\\\ \n\\frac{\\partial h_n(\\bx)}{\\partial x_j}\n\\end{array} \\right), \\text{ respectively},\n$$\n\nwe can rewrite the equation as\n\n$$\n\\frac{\\partial f(\\bx)}{\\partial x_j} = \\frac{\\partial \\g(\\bx)}{\\partial x_j} ^T \\h(\\bx) + \n\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}  = \\h(\\bx)^T\\frac{\\partial \\g(\\bx)}{\\partial x_j}\n+\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}\n$$\n\nFinally,\n\n$$\n\\bnabla f(\\bx)^T = f'(\\bx) = \\h(\\bx)^T \\g'(\\bx) + \\g(\\bx)^T\\h'(\\bx).\n$$\n\n$$\n\\implies \\bnabla f(\\bx) = \\bnabla g(\\bx) \\ \\h(\\bx) + \\bnabla h(\\bx) \\ \\g(\\bx).\n$$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Derivative of quadratic form\nIf $\\A\\in\\R^{n\\times n}$ is a symmetric matrix, the function\n\n$$\nf:\\R^n\\rightarrow \\R,\\quad\\bx\\mapsto f(\\bx) = \\bx^T\\,\\A\\,\\bx\n$$ is called a quadratic form. <br>\n<br>\nWith the definitions,\n\n$$\n\\g(\\bx) := \\bx \\ \\text{and } \\ \\h(\\bx):= \\A\\,\\bx,\n$$\n\nwe have $f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, i.e., exactly the situation as above.\nWith $\\g'(\\bx) = \\mathbf{ I}$, where $\\mathbf{ I}$ denotes the unity matrix, and $\\h'(\\bx) = \\A$, it is more or less easy to see that the gradient of $f$ is given by\n\n$$\n\\bnabla f(\\bx) = 2\\,\\A\\,\\bx.\n$$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Example 1\nLet the function $f:\\R^2 \\rightarrow \\R$ be defined as\n$ f(x,y) = \\frac{1}{2} (x^2 + \\alpha y^2), \\alpha \\in \\R $\n\n<u>Task 1</u>: Find all stationary points of the function $f$.\n\nTaking partial derivatives with respect to x we get $x$, with respect to y we get $\\alpha y$. Stationary points are those at which the gradient is 0, so $x = 0$, and $y$ is either 0 if $\\alpha != 0$ or it can assume any value if $\\alpha = 0$. Stationary points are on the line (0, t)\n\n\n<u>Task 2</u>: Calculate the Hessian of the function $f$ for arbitrary $x$ and $y$.\n\nThe Hessian is a matrix of partial double derivatives, so it is 1 and $\\alpha$ on the diagonal, and 0 and 0 in the rest of the 2x2 matrix\n\n\n<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$ with respect to $x$ and $y$?\n\n1 and $\\alpha$\n\n<u>Task 4</u>: Characterize the stationary points for positive and negative $\\alpha$.\n\nif $\\alpha \\lt 0$ => $H$ is indefinite => (0,0) is a saddle point\nif $\\alpha \\gt 0$ => $H$ is positive definite => (0,0) is the minimum\n\n<u>Task 5</u>: Characterize the convexity of the function for every $\\alpha$.\n\nif $\\alpha \\lt 0$ => $H$ is indefinite => non-convex\nif $\\alpha = 0$ => $H$ is positive semidefinite => convex\nif $\\alpha \\gt 0$ => $H$ is positive definite => strictly ocnvex",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Example 2: Rosenbrock function\nThe Rosenbrock function is a famous test function used in optimization. <br>\n<br>\nIt is defined by: \n$f:\\R^2 \\rightarrow \\R$, $ f(x,y) = (a-x)^2 + b(y-x^2)^2 $ with $a=1, b=100$\n\n<u>Task 1</u>: Find all stationary points of the function $f$.\n\n<u>Task 2</u>: Calculate the Hessian of the function $f$ for the stationary points found. (Hint: use Python)\n\n<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$? (Hint: use Python)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def rosenbrock(x):\n    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# compute hessian using autograd\nimport autograd\nimport numpy as np\nx0 = np.array([1.0, 1.0])\nhessian_rosenbrock = autograd.hessian(rosenbrock)(x0)\nhessian_rosenbrock",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[ 802., -400.],\n       [-400.,  200.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# compute the eigenvalues using numpy\nimport numpy as np\nvals, vecs = np.linalg.eig(hessian_rosenbrock)\nprint(vals)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1.00160064e+03 3.99360767e-01]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}