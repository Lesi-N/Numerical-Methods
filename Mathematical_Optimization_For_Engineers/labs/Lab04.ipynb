{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<div>\n<img src=\"figures/svtLogo.png\"/>\n</div>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<center><h1>Mathematical Optimization for Engineers</h1></center>\n<center><h2>Lab 4 - Convergence rates</h2></center>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let the function $f:\\mathcal R^2  \\rightarrow \\mathcal R$ be defined as\n$$f(x,y) = x^2+y^3$$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 1</u>: Calculate the first derivative of the function for arbitrary $x$ and $y$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 2</u>: Calculate the coordinates of the stationary point.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 3</u>: Calculate the Hessian of the function for arbitrary $x$ and $y$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 4</u>: What are the Eigenvalues of the Hessian for arbitrary $x$ and $y$?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 5</u>: Check the second-order necessary and the second-order sufficient conditions of the stationary point.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 6</u>: Characterize the stationary point.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this lab, you have the opportunity to experiment with different solvers for unconstrained optimization.\n\nWe have implemented the following solvers for you:<br><br>\n(a) steepest_descent with armijo step-length<br>\n(b) steepest_descent with wolfe step-length<br>\n(c) steepest_descent with constant step-length<br>\n(d) newton's method<br>\n(e) inexact newton's method with CG (from scipy)<br>\n(f) CG method (from scipy)<br>\n(g) bfgs method (from scipy)\n\nMoreover, you can test these methods on several different objective functions defined below.\n\nTry to answer the following questions:<br>\n1. What happens when you change the initial point? and why? <br>\n\nUnocnstrained optimization can only find a local optimum. If we change the point, the method might find a differetn optimum for our non-convex functions, but it might not be a better solution.\n\n2. Are the given problems convex or non-convex? <br>\n\nNon-convex\n\n3. By looking at the convergence plots, can you identify linear, order-p or super-linear convergence rates?\n\nLinear convergence: steepest descent\nOrder-p: Newton's nethod, CG (could be super-linear as we cannot always tell just from the graph)\n\n4. Which parameters can you change in different solvers to improve/worsen their respective performances?\n\nNewton's method can be substituted for other versions, or for steepest descent. Steepest descent line search parameters lead to different results.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Imports",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import autograd.numpy as np\n\nimport scipy.optimize as minimize\n\nfrom numpy.linalg import inv\n\nfrom autograd import grad\nfrom autograd import hessian\nfrom autograd import elementwise_grad as egrad\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\nimport matplotlib.ticker",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Plot",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# You can ignore this cell. It only serves to create nice plots.\ndef plot_optimization(func, xIter):\n    \n    x1 = []\n    x2 = []\n    f = []\n    \n    x1 = [elx[0] for elx in xIter]\n    x2 = [ely[1] for ely in xIter]\n    f = [func([i,j]) for i,j in zip(x1,x2)]\n    \n    fig = plt.figure(figsize=(10,10)) # create figure\n    ax = fig.add_subplot(111,projection='3d' )\n    ax.view_init(elev=30, azim=30)\n    \n    ax.plot(x1, x2, f, linestyle=\"dashed\",linewidth=1, color=\"red\")\n\n    for it in range(len(f)):\n        ax.scatter(x1[it],x2[it],f[it], marker='x', color=\"red\")\n    \n    xmax = max(x1)\n    xmin = min(x1)\n    ymax = max(x2)\n    ymin = min(x2)\n    \n    X = np.linspace(xmin-1, xmax+1)\n    Y = np.linspace(ymin-1, ymax+1)\n    X, Y = np.meshgrid(X, Y)\n    \n    zs = np.array(func([np.ravel(X), np.ravel(Y)]))\n    Z = zs.reshape(X.shape)\n    \n    # Surface plot:\n    ax.plot_surface(X, Y, Z, cmap=plt.cm.jet, antialiased=True,cstride=1,rstride=1, alpha=0.75)\n\n    plt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Objective functions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def sixhump(x): #scipy-lectures.org/intro/scipy/auto_examples/plot_2d_minimization.html\n    return ((4 - 2.1*x[0]**2 + x[0]**4 / 3.) * x[0]**2 + x[0] * x[1] + (-4 + 4*x[1]**2) * x[1] **2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def rosen(x): #scipy-lectures.org/intro/scipy/auto_examples/plot_2d_minimization.html\n    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def beale(X):\n    x, y = X[0], X[1]\n    return ((1.500 - x + x*y)**2 + (2.250 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def adjiman (X):\n    x, y = X[0], X[1]\n    return (np.cos(x) * np.sin(y)) - (x / (y**2.0 + 1.0))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Gradient",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def gradient_function(func, x): \n    return grad(func)(x)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Hessian",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def hessian_function(func,x):\n    return hessian(func)(x)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Steepest descent",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\ndef steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n    \n    xCur = x0\n    fcur = function(xCur)\n    gCur = gradient_function(function,xCur)\n    \n    # norm of the gradient at initial point for optimality condition\n    nmg0 = np.linalg.norm(gradient_function(function, x0))\n    \n    # count number of steps taken by method\n    it = 0\n    \n    # accumulate number of iterations needed by linesearch algorithm in every step\n    ls_iter = 0\n    \n    # store iterates for plotting\n    xIter=[]\n    xIter.append(x0)\n    \n    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n\n        it=it+1\n        \n        # calculate descent direction\n        direction = -1.0 * gradient_function(function,xCur)\n        \n        # calculate step-length\n        if (stepRule=='armijo'):\n            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n        elif (stepRule=='wolfe'):\n            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n        else:\n            alpha = 0.001\n            ls_ = 1\n        ls_iter = ls_iter + ls_\n        \n        # update current point\n        xCur = xCur + alpha * direction\n        gCur = gradient_function(function,xCur)\n        fcur = function(xCur)\n        xIter.append(xCur)\n\n    return xCur, fcur, it, xIter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Newton's method",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# input: (objective function, initial guess, optimality tolerance)\ndef newton_descent(function, x0, tol=0.01): \n\n    xCur = x0\n    fcur = function(xCur)\n    gCur = gradient_function(function,xCur)\n    hCur = hessian_function(function,xCur)    \n    \n    # norm of the gradient at initial point for optimality condition\n    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n    \n    # count number of steps taken by method    \n    it = 0\n    \n    # store iterates for plotting    \n    xIter=[]\n    xIter.append(x0)\n    \n    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n        \n        it=it+1\n        \n        # calculate descent direction\n        direction = -1.0 * np.dot(inv(hCur),gCur)\n        \n        # calculate step-length\n        alpha = 1.0\n        \n        # update current point\n        xCur = xCur + alpha * direction\n        fcur = function(xCur)        \n        gCur = gradient_function(function,xCur)\n        hCur = hessian_function(function,xCur)\n        xIter.append(xCur)\n\n    return xCur, fcur, it, xIter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### BFGS (scipy)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def bfgs_scipy(func, x0):\n\n    global Nfeval\n    global xIter    \n    Nfeval = 0\n    xIter=[]\n    xIter.append(x0)\n    \n    def callbackF(Xi):\n\n        global Nfeval\n        global xIter \n        Nfeval += 1    \n        xIter.append(Xi)\n    \n    [xopt, fopt, gopt, Bopt, func_calls, grad_calls, warnflg] = \\\n        minimize.fmin_bfgs(func, x0, callback=callbackF, maxiter=2000, full_output=True, disp=False, retall=False)\n    \n    return xopt, fopt, Nfeval, xIter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### CG (scipy)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def cg_scipy(func, x0):\n\n    global Nfeval\n    global xIter    \n    Nfeval = 0\n    xIter=[]\n    xIter.append(x0)\n    \n    def callbackF(Xi):\n\n        global Nfeval\n        global xIter \n        Nfeval += 1    \n        xIter.append(Xi)\n    \n    [xopt, fopt, func_calls, grad_calls, warnflg] = \\\n        minimize.fmin_cg(func, x0, callback=callbackF, maxiter=2000, full_output=True, disp=False, retall=False)\n    \n    return xopt, fopt, Nfeval, xIter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Newton with CG (scipy)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def ncg_scipy(func, x0):\n\n    global Nfeval\n    global xIter    \n    Nfeval = 0\n    xIter=[]\n    xIter.append(x0)\n    \n    def callbackF(Xi):\n\n        global Nfeval\n        global xIter \n        Nfeval += 1    \n        xIter.append(Xi)\n        \n    fprime = grad(func)\n    fhess = hessian(func)\n    \n    [xopt, fopt, func_calls, grad_calls, hess_calls,  warnflg] = \\\n        minimize.fmin_ncg(func, x0, fprime, callback=callbackF, full_output=True, disp=False, retall=False)\n    \n    return xopt, fopt, Nfeval, xIter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Armijo",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def armijo(function, xcur, searchdirection, c1, alpha0): \n    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n    \n    alpha = alpha0\n    xnew = xcur + alpha * searchdirection\n    fcur = function(xcur)\n    fnew = function(xnew)\n    gradientCur = gradient_function(function,xcur)\n    numiter = 0\n    # check for Armijo condition\n    while ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))): \n        numiter += 1\n        alpha = alpha/2.0\n        xnew = xcur + alpha * searchdirection\n        fnew = function(xnew)\n\n    return alpha, numiter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Wolfe",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n    \n    alpha = alpha0\n    xnew = xcur + alpha * searchdirection\n    fcur = function(xcur)\n    fnew = function(xnew)\n    gradientCur = gradient_function(function,xcur)\n    gradientNew = gradient_function(function,xnew)\n    numiter = 0\n    \n    lb = 0\n    ub = np.inf \n    \n    # check for Wolfe conditions\n    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n    while True: \n        numiter += 1\n        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n            ub = alpha\n            alpha = 0.5 * (lb + ub)\n        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n        #elif (np.abs(np.dot(gradientNew,searchdirection)) > c2 * np.abs(np.dot(gradientCur,searchdirection))):\n            lb = alpha\n            if np.isinf(ub):\n                alpha = 2 * lb\n            else:\n                alpha = 0.5 * (lb + ub)\n        else:\n            break\n        xnew = xcur + alpha * searchdirection\n        fnew = function(xnew)\n        gradientNew = gradient_function(function,xnew)\n\n    return alpha, numiter",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Calls Rosenbrock",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# initial point\nx0 = np.array([-2., 2.])\n\nc1 = 0.5 \nc2 = 0.6 \nalpha0 = 0.125 \ntol = 0.0001\n\nnum_last_iters = 3\nxoptStar = [1.0, 1.0]  # optimal point\n\n# a: steepest_descent armijo\n# b: steepest_descent wolfe\n# c: steepest_descent constant\n# d: newton\n# e: ncg_scipy\n# f: cg_scipy\n# g: bfgs_scipy",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# calling all solvers\nxopt_a, fopt_a, it_a, xIter_a = steepest_descent(rosen, x0, 'armijo', c1, c2, alpha0)\nxopt_b, fopt_b, it_b, xIter_b = steepest_descent(rosen, x0, 'wolfe', c1, c2, alpha0)\nxopt_c, fopt_c, it_c, xIter_c = steepest_descent(rosen, x0, 'constant', c1, c2, alpha0)\nxopt_d, fopt_d, it_d, xIter_d = newton_descent(rosen, x0)\nxopt_e, fopt_e, it_e, xIter_e = ncg_scipy(rosen, x0)\nxopt_f, fopt_f, it_f, xIter_f = cg_scipy(rosen, x0)\nxopt_g, fopt_g, it_g, xIter_g = bfgs_scipy(rosen, x0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# plotting\n# you can ignore this cell\n# here, we simply plot convergence on y-axis v/s the last four iterations of the solver\ndiff_a = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_a]\ndiff_b = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_b]\ndiff_c = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_c]\ndiff_d = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_d]\ndiff_e = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_e]\ndiff_f = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_f]\ndiff_g = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_g]\n\nfig, ax = plt.subplots(4, 2, figsize=(20,20))\nfig.text(0.5, 0.04, 'iteration',fontsize = 24, ha='center')\nfig.text(0.04, 0.5, '$||x^k - x^*||$',fontsize = 24, va='center', rotation='vertical')\n\nax[0, 0].plot(list(range(it_a-num_last_iters, it_a+1)),diff_a[it_a-num_last_iters:it_a+1])\nax[0, 1].plot(list(range(it_b-num_last_iters, it_b+1)),diff_b[it_b-num_last_iters:it_b+1])\nax[1, 0].plot(list(range(it_c-num_last_iters, it_c+1)),diff_c[it_c-num_last_iters:it_c+1])\nax[1, 1].plot(list(range(it_d-num_last_iters, it_d+1)),diff_d[it_d-num_last_iters:it_d+1])\nax[2, 0].plot(list(range(it_e-num_last_iters, it_e+1)),diff_e[it_e-num_last_iters:it_e+1])\nax[2, 1].plot(list(range(it_f-num_last_iters, it_f+1)),diff_f[it_f-num_last_iters:it_f+1])\nax[3, 0].plot(list(range(it_g-num_last_iters, it_g+1)),diff_g[it_g-num_last_iters:it_g+1])\nax[3, 1].axis('off')\n\nax[0, 0].set_xticks(np.arange(it_a-num_last_iters, it_a+1, 1))\nax[0, 1].set_xticks(np.arange(it_b-num_last_iters, it_b+1, 1))\nax[1, 0].set_xticks(np.arange(it_c-num_last_iters, it_c+1, 1))\nax[1, 1].set_xticks(np.arange(it_d-num_last_iters, it_d+1, 1))\nax[2, 0].set_xticks(np.arange(it_e-num_last_iters, it_e+1, 1))\nax[2, 1].set_xticks(np.arange(it_f-num_last_iters, it_f+1, 1))\nax[3, 0].set_xticks(np.arange(it_g-num_last_iters, it_g+1, 1))\n\nax[0, 0].title.set_text('steepest_descent armijo')\nax[0, 1].title.set_text('steepest_descent wolfe')\nax[1, 0].title.set_text('steepest_descent constant')\nax[1, 1].title.set_text('newton descent')\nax[2, 0].title.set_text('ncg_scipy')\nax[2, 1].title.set_text('cg_scipy')\nax[3, 0].title.set_text('bfgs_scipy')\n\nax[0, 0].set_yscale('log')\nax[0, 1].set_yscale('log')\nax[1, 0].set_yscale('log')\nax[1, 1].set_yscale('log')\nax[2, 0].set_yscale('log')\nax[2, 1].set_yscale('log')\nax[3, 0].set_yscale('log')\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Adjiman function",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# initial point\nx0 = np.array([-2., 2.])\n\nc1 = 0.5 \nc2 = 0.6 \nalpha0 = 0.125 \ntol = 0.0001\n\nnum_last_iters = 3\nxoptStar = np.array([-2.92293048, 2.04457258])  # approx optimum\n\n# a: steepest_descent armijo\n# b: steepest_descent wolfe\n# c: steepest_descent constant\n# d: newton\n# e: ncg_scipy\n# f: cg_scipy\n# g: bfgs_scipy",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# calling all solvers\nxopt_a, fopt_a, it_a, xIter_a = steepest_descent(adjiman, x0, 'armijo', c1, c2, alpha0)\nxopt_b, fopt_b, it_b, xIter_b = steepest_descent(adjiman, x0, 'wolfe', c1, c2, alpha0)\nxopt_c, fopt_c, it_c, xIter_c = steepest_descent(adjiman, x0, 'constant', c1, c2, alpha0)\nxopt_d, fopt_d, it_d, xIter_d = newton_descent(adjiman, x0)\nxopt_e, fopt_e, it_e, xIter_e = ncg_scipy(adjiman, x0)\nxopt_f, fopt_f, it_f, xIter_f = cg_scipy(adjiman, x0)\nxopt_g, fopt_g, it_g, xIter_g = bfgs_scipy(adjiman, x0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# plotting\n# you can ignore this cell\n# here, we simply plot convergence on y-axis v/s the last four iterations of the solver\ndiff_a = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_a]\ndiff_b = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_b]\ndiff_c = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_c]\ndiff_d = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_d]\ndiff_e = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_e]\ndiff_f = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_f]\ndiff_g = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_g]\n\nfig, ax = plt.subplots(4, 2,figsize=(20,20))\nfig.text(0.5, 0.04, 'iteration',fontsize = 24, ha='center')\nfig.text(0.04, 0.5, '$||x^k - x^*||$',fontsize = 24, va='center', rotation='vertical')\n\nax[0, 0].plot(list(range(it_a-num_last_iters, it_a+1)),diff_a[it_a-num_last_iters:it_a+1])\nax[0, 1].plot(list(range(it_b-num_last_iters, it_b+1)),diff_b[it_b-num_last_iters:it_b+1])\nax[1, 0].plot(list(range(it_c-num_last_iters, it_c+1)),diff_c[it_c-num_last_iters:it_c+1])\nax[1, 1].plot(list(range(it_d-num_last_iters, it_d+1)),diff_d[it_d-num_last_iters:it_d+1])\nax[2, 0].plot(list(range(it_e-num_last_iters, it_e+1)),diff_e[it_e-num_last_iters:it_e+1])\nax[2, 1].plot(list(range(it_f-num_last_iters, it_f+1)),diff_f[it_f-num_last_iters:it_f+1])\nax[3, 0].plot(list(range(it_g-num_last_iters, it_g+1)),diff_g[it_g-num_last_iters:it_g+1])\nax[3, 1].axis('off')\n\nax[0, 0].set_xticks(np.arange(it_a-num_last_iters, it_a+1, 1))\nax[0, 1].set_xticks(np.arange(it_b-num_last_iters, it_b+1, 1))\nax[1, 0].set_xticks(np.arange(it_c-num_last_iters, it_c+1, 1))\nax[1, 1].set_xticks(np.arange(it_d-num_last_iters, it_d+1, 1))\nax[2, 0].set_xticks(np.arange(it_e-num_last_iters, it_e+1, 1))\nax[2, 1].set_xticks(np.arange(it_f-num_last_iters, it_f+1, 1))\nax[3, 0].set_xticks(np.arange(it_g-num_last_iters, it_g+1, 1))\n\nax[0, 0].title.set_text('steepest_descent armijo')\nax[0, 1].title.set_text('steepest_descent wolfe')\nax[1, 0].title.set_text('steepest_descent constant')\nax[1, 1].title.set_text('newton descent')\nax[2, 0].title.set_text('ncg_scipy')\nax[2, 1].title.set_text('cg_scipy')\nax[3, 0].title.set_text('bfgs_scipy')\n\nax[0, 0].set_yscale('log')\nax[0, 1].set_yscale('log')\nax[1, 0].set_yscale('log')\nax[1, 1].set_yscale('log')\nax[2, 0].set_yscale('log')\nax[2, 1].set_yscale('log')\nax[3, 0].set_yscale('log')\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}