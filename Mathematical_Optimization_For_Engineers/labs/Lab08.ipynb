{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<div>\n<img src=\"figures/svtLogo.png\"/>\n</div>  \n\n<center><h1>Mathematical Optimization for Engineers</h1></center>\n<center><h2>Lab 8 - Elimination of variables, Penalty and SQP methods</h2></center>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "$$\\newcommand{\\bx}{\\mathbf{x}}$$\nThe following problem is given:\n\\begin{align*}\n  \\min_{\\bx \\in \\mathbb{R}^2} \\;\\; & f(\\bx)  \\\\\n   \\text{s.t.} \\;\\;& x_1+x_2=8,\n\\end{align*}\n\nwhere $f(\\bx) = - (x_1^2+x_2^2+4x_1x_2)$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 1</u>: Find the minimum of the function using variable elimination.\nCheck the second-order sufficient conditions for the unconstrained one-variable problem.\n\n\nSolving for x1: x1 = 8 - x2 <br>\n\nf(x1,x2) = −(8 − x2)^2 − (x2)^2 − 4(8−x2)x2 = 2(x2)^2 − 16x2 − 64 <br>\n\nDefine S(x2) = (x2)^2 - 8x2 <br>\nOptimization problem: min (x2)^2 - 8x2 <br>\n<br>\nStationary point: <br>\nS'(x2) = 2x2 - 8 = 0 => x <br>\nx1 = 8 - x2 => x1 = 4\n\nThe Hessian S_x2,x2 = 2, second-order conditions satisfied\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Another possibility is to use the following penalty function:\n\\begin{align*}\n  \tQ(\\bx;\\mu)=f(\\bx)+\\frac{1}{2\\mu} (x_1+x_2-8)^2\\,,\n\\end{align*}\nwith $\\mu>0$ being a penalty parameter.\n     \n<br>\n<u>Task 2</u>: Write down the first-order necessary condition of optimality for minimizing $Q$.\n\nPartial derivative for x1: <br>\nf_x1 = -2x1 - 4x2 + 1/$\\mu$(x1 + x2 - 8)\\*1 <br>\n\nPartial derivative for x1: <br>\nf_x2 = -2x2 - 4x1 + 1/$\\mu$(x1 + x2 - 8)\\*1 <br>\n\nf_x1 = 0, f_x2 = 0 <br>\n\n-2x1 - 4x2 + 1/$\\mu$(x1 + x2 - 8) = 0 <br>\n-2x2 - 4x1 + 1/$\\mu$(x1 + x2 - 8) = 0 <br>\n\n\nx1(1/$\\mu$ - 2) + x2(1/$\\mu$ - 4) = 8/$\\mu$ <br>\nx1(1/$\\mu$ - 4) + x2(1/$\\mu$ - 2) = 8/$\\mu$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<u>Task 3</u>: What happens as $\\mu \\rightarrow 0$?  Complete the implementation of the quadratic penalty method below:\n\nAlso, report the eigenvalues and the condition number of the Hessian for each $\\mu$.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport scipy.optimize as sp\n\n# to calculate the gradient and Hessian of the objective function\nfrom autograd import grad\nfrom autograd import hessian\n\n# to solve additionally using SLSQP solver, later on\nfrom scipy.optimize import Bounds\nfrom scipy.optimize import NonlinearConstraint\nfrom math import inf",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Objective, constraint, quadratic penalty function, gradient and hessian",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def objective(X):\n    x1, x2 = X[0], X[1]\n    f = -(x1 ** 2 + x2 ** 2 + 4 * x1 * x2)\n    return f",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def constraint(X):\n    x1, x2 = X[0], X[1]\n    c = x1 + x2 - 8\n    return c",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def penaltyFunction(X, mu):\n    x1, x2 = X[0], X[1]\n    f = -(x1 ** 2 + x2 ** 2 + 4 * x1 * x2) + 1 / (2 * mu) * (x1 + x2 - 8) ** 2\n    return f",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def gradient_function(x, mu): \n    return [el.item() for el in grad(penaltyFunction, 0)(x, mu)]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def hessian_function(x, mu): \n    return hessian(penaltyFunction, 0)(x, mu)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Quadratic penalty method",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def qpm(x0, mu): \n    \n    # get eigenvalues of the Hessian\n    w, v = np.linalg.eig(hessian_function(x0, mu))\n    \n    # get condition number of the Hessian\n    n = np.linalg.cond(hessian_function(x0, mu))\n    \n    # unconstrained optimization using BFGS method\n    res = sp.minimize(penaltyFunction, x0, args=(mu), method='BFGS', jac=gradient_function)\n    \n    return w, n, res.x",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mu = 1\nx0 = np.array([1.,1.])\n\n# acceptable constraint violation at optimum\neps_viol = 1e-15\nconstraint_violation = True\n\nit = 0\n\nprint (\"{:<10} {:<10} {:<20} {:^20} {:^30}\".format('iter','mu','minimum','condition nr.', 'constraint violation'))\nwhile constraint_violation:\n    it = it + 1\n    \n    w, n, xmin = qpm(x0,mu)\n    print (\"{:<10d} {:<10.3e} [{:^8.4f}, {:^8.4f}] {:<4} {:<20.2e} {:^20.3e}\".format(it,mu,xmin[0],xmin[1],' ',n,constraint(xmin)))\n    \n    if constraint(xmin) <= eps_viol:\n        constraint_violation = False \n    \n    # update for next iteration (e.g. half of previous penalty value)\n    mu = mu/2\n    x0 = xmin",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### SLSQP method (scipy)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We will solve the problem using scipy's SLSQP solver (written by Dieter Kraft, DLR Oberpfaffenhofen)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x0 = np.array([0.,0.])\n\nbounds = Bounds([-inf,-inf], [inf,inf])\n\n# The constraint is actually linear, so you can also try a different approach.\n# See SLSQP documentation for more details on how to set up linear constraints.\nnonlinear_constraints = NonlinearConstraint(constraint, 0, 0)\n\n# use SLSQP\nres = sp.minimize(objective, x0, method='SLSQP',\n               constraints=[nonlinear_constraints], bounds=bounds, \n                  options={'disp': True, 'iprint': 4} )\n\nprint(\"minimum = {}\".format(res.x))\nprint(\"constraint violation = {}\".format(constraint(res.x)))\n# The problem is a QP which is the reason why the SLSQP method is so fast. ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Rosenbrock function contrained\n\nThe original Rosenbrock function does not have constraints, however, we introduce a constraint \n$$x_1^2 + x_2^2 - 1 \\leq 0$$",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def rosenbrock(x):\n    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def rosenbrock_inequality(x): \n    return x[0]**2 + x[1]**2 - 1",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def penalty_inequality(x, mu): \n    f = (x[0]-1)**2 + 100*(x[1]-x[0]**2)**2 + (x[0]**2 + x[1]**2 - 1)**2/(2*mu)\n    return f",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def qpm_inequality(x0, mu): \n    \n    # get eigenvalues of the Hessian\n    #w, v = np.linalg.eig(hessian_function(x0, mu))\n    \n    # get condition number of the Hessian\n    #n = np.linalg.cond(hessian_function(x0, mu))\n    \n    # unconstrained optimization using BFGS method\n    res = sp.minimize(penalty_inequality, x0, args=(mu), method='BFGS')# jac=gradient_function)\n\n    return res.x\n    #return w, n, res.x",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mu = 1\nx0 = np.array([0.0, 0.0])\n\n# acceptable constraint violation at optimum\neps_f = 1e-8\nsufficient_decrease = True\n\nit = 0\n\nprint (\"{:<10} {:<10} {:<20} {:^20} {:^30}\".format('iter','mu','minimum','condition nr.', 'constraint value'))\nwhile sufficient_decrease:\n    it = it + 1\n    f_prev = rosenbrock(x0)\n    xmin = qpm_inequality(x0, mu)\n    print (\"{:<10d} {:<10.3e} [{:^8.4f}, {:^8.4f}] {:<4} {:<20.2e} {:^20.3e}\".format(it,mu,xmin[0],xmin[1],' ',n, rosenbrock_inequality(xmin)))\n    \n    if abs(f_prev - rosenbrock(xmin)) <= eps_f:\n        sufficient_decrease = False \n    \n    # update penalty for next iteration (e.g. half of previous value)\n    mu = mu/2\n    x0 = xmin\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x0 = np.array([0.,0.])\n\nbounds = Bounds([-inf,-inf], [inf,inf])\n\nnonlinear_constraints = NonlinearConstraint(rosenbrock_inequality, -inf, 0)\n\n# use SLSQP\nres = sp.minimize(rosenbrock, x0, method='SLSQP',\n               constraints=[nonlinear_constraints], bounds=bounds, options={'disp': True, 'iprint': 4} )\n\nprint(\"minimum = {}\".format(res.x))\nprint(\"constraint value = {}\".format(rosenbrock_inequality(res.x)))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}